{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install attridict\n!pip install indic-numtowords\n!pip install g2p_en==2.1.0\n!pip install espnet\n!pip install -U librosa\n!pip install indic_unified_parser","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:28:15.123175Z","iopub.execute_input":"2024-08-24T01:28:15.123652Z","iopub.status.idle":"2024-08-24T01:31:25.723539Z","shell.execute_reply.started":"2024-08-24T01:28:15.123618Z","shell.execute_reply":"2024-08-24T01:31:25.722172Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting attridict\n  Downloading attridict-0.0.9-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: PyYAML==6.0.1 in /opt/conda/lib/python3.10/site-packages (from attridict) (6.0.1)\nDownloading attridict-0.0.9-py3-none-any.whl (5.9 kB)\nInstalling collected packages: attridict\nSuccessfully installed attridict-0.0.9\nCollecting indic-numtowords\n  Downloading indic_numtowords-1.0.2.tar.gz (24 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: indic-numtowords\n  Building wheel for indic-numtowords (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for indic-numtowords: filename=indic_numtowords-1.0.2-py3-none-any.whl size=39213 sha256=b1270b0ad7f4977c57e1431f7e8592e5439363352be10be5fc86091ab606e3a4\n  Stored in directory: /root/.cache/pip/wheels/d5/91/21/335088ba9732509344fa6b16cc4dc21be92553d586bfa2f401\nSuccessfully built indic-numtowords\nInstalling collected packages: indic-numtowords\nSuccessfully installed indic-numtowords-1.0.2\nCollecting g2p_en==2.1.0\n  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: numpy>=1.13.1 in /opt/conda/lib/python3.10/site-packages (from g2p_en==2.1.0) (1.26.4)\nRequirement already satisfied: nltk>=3.2.4 in /opt/conda/lib/python3.10/site-packages (from g2p_en==2.1.0) (3.2.4)\nCollecting inflect>=0.3.1 (from g2p_en==2.1.0)\n  Downloading inflect-7.3.1-py3-none-any.whl.metadata (21 kB)\nCollecting distance>=0.1.3 (from g2p_en==2.1.0)\n  Downloading Distance-0.1.3.tar.gz (180 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: more-itertools>=8.5.0 in /opt/conda/lib/python3.10/site-packages (from inflect>=0.3.1->g2p_en==2.1.0) (10.2.0)\nRequirement already satisfied: typeguard>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from inflect>=0.3.1->g2p_en==2.1.0) (4.1.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk>=3.2.4->g2p_en==2.1.0) (1.16.0)\nRequirement already satisfied: typing-extensions>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from typeguard>=4.0.1->inflect>=0.3.1->g2p_en==2.1.0) (4.9.0)\nDownloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading inflect-7.3.1-py3-none-any.whl (34 kB)\nBuilding wheels for collected packages: distance\n  Building wheel for distance (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=c5335aa89a572a556629dd411da37d8a3ab1d0dd3a8b9cf9f7d9dea67aabac11\n  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\nSuccessfully built distance\nInstalling collected packages: distance, inflect, g2p_en\nSuccessfully installed distance-0.1.3 g2p_en-2.1.0 inflect-7.3.1\nCollecting espnet\n  Downloading espnet-202402-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m703.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools>=38.5.1 in /opt/conda/lib/python3.10/site-packages (from espnet) (69.0.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from espnet) (21.3)\nCollecting configargparse>=1.2.1 (from espnet)\n  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\nCollecting typeguard==2.13.3 (from espnet)\n  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\nCollecting humanfriendly (from espnet)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from espnet) (1.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from espnet) (3.13.1)\nCollecting librosa==0.9.2 (from espnet)\n  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\nCollecting jamo==0.4.1 (from espnet)\n  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: PyYAML>=5.1.2 in /opt/conda/lib/python3.10/site-packages (from espnet) (6.0.1)\nRequirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.10/site-packages (from espnet) (0.12.1)\nRequirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from espnet) (3.10.0)\nCollecting kaldiio>=2.18.0 (from espnet)\n  Downloading kaldiio-2.18.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from espnet) (2.1.2)\nCollecting torch-complex (from espnet)\n  Downloading torch_complex-0.4.4-py3-none-any.whl.metadata (3.1 kB)\nCollecting nltk>=3.4.5 (from espnet)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting numpy<1.24 (from espnet)\n  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from espnet) (3.20.3)\nCollecting hydra-core (from espnet)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from espnet) (3.3.0)\nCollecting sentencepiece==0.1.97 (from espnet)\n  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting ctc-segmentation>=1.6.6 (from espnet)\n  Downloading ctc_segmentation-1.7.4.tar.gz (73 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting pyworld>=0.3.4 (from espnet)\n  Downloading pyworld-0.3.4.tar.gz (251 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting pypinyin<=0.44.0 (from espnet)\n  Downloading pypinyin-0.44.0-py2.py3-none-any.whl.metadata (10 kB)\nCollecting espnet-tts-frontend (from espnet)\n  Downloading espnet_tts_frontend-0.0.3-py3-none-any.whl.metadata (3.4 kB)\nCollecting ci-sdr (from espnet)\n  Downloading ci_sdr-0.0.2.tar.gz (15 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fast-bss-eval==0.1.3 (from espnet)\n  Downloading fast_bss_eval-0.1.3.tar.gz (33 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting asteroid-filterbanks==0.4.0 (from espnet)\n  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting editdistance (from espnet)\n  Downloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nCollecting importlib-metadata<5.0 (from espnet)\n  Downloading importlib_metadata-4.13.0-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from asteroid-filterbanks==0.4.0->espnet) (4.9.0)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2->espnet) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2->espnet) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2->espnet) (1.4.2)\nRequirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2->espnet) (5.1.1)\nCollecting resampy>=0.2.2 (from librosa==0.9.2->espnet)\n  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2->espnet) (0.58.1)\nRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2->espnet) (1.8.2)\nRequirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from ctc-segmentation>=1.6.6->espnet) (3.0.8)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<5.0->espnet) (3.17.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.4.5->espnet) (8.1.7)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.4.5->espnet) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.4.5->espnet) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->espnet) (3.1.1)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.10.2->espnet) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->espnet) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->espnet) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->espnet) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->espnet) (2024.5.0)\nCollecting einops (from ci-sdr->espnet)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting unidecode>=1.0.22 (from espnet-tts-frontend->espnet)\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: inflect>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from espnet-tts-frontend->espnet) (7.3.1)\nCollecting jaconv (from espnet-tts-frontend->espnet)\n  Downloading jaconv-0.4.0.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: g2p-en in /opt/conda/lib/python3.10/site-packages (from espnet-tts-frontend->espnet) (2.1.0)\nCollecting omegaconf<2.4,>=2.2 (from hydra-core->espnet)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core->espnet)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.10.2->espnet) (2.21)\nRequirement already satisfied: more-itertools>=8.5.0 in /opt/conda/lib/python3.10/site-packages (from inflect>=1.0.0->espnet-tts-frontend->espnet) (10.2.0)\nINFO: pip is looking at multiple versions of inflect to determine which version is compatible with other requirements. This could take a while.\nCollecting inflect>=1.0.0 (from espnet-tts-frontend->espnet)\n  Downloading inflect-7.3.0-py3-none-any.whl.metadata (21 kB)\n  Downloading inflect-7.2.1-py3-none-any.whl.metadata (21 kB)\n  Downloading inflect-7.2.0-py3-none-any.whl.metadata (21 kB)\n  Downloading inflect-7.0.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pydantic>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from inflect>=1.0.0->espnet-tts-frontend->espnet) (2.5.3)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.45.1->librosa==0.9.2->espnet) (0.41.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.2->espnet) (3.11.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.2->espnet) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->librosa==0.9.2->espnet) (3.2.0)\nRequirement already satisfied: distance>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from g2p-en->espnet-tts-frontend->espnet) (0.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->espnet) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->espnet) (1.3.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=1.0.0->espnet-tts-frontend->espnet) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=1.0.0->espnet-tts-frontend->espnet) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2->espnet) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2->espnet) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2->espnet) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2->espnet) (2024.7.4)\nDownloading espnet-202402-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\nDownloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\nDownloading librosa-0.9.2-py3-none-any.whl (214 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\nDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\nDownloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\nDownloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypinyin-0.44.0-py2.py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading espnet_tts_frontend-0.0.3-py3-none-any.whl (11 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch_complex-0.4.4-py3-none-any.whl (9.1 kB)\nDownloading inflect-7.0.0-py3-none-any.whl (34 kB)\nDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fast-bss-eval, ctc-segmentation, pyworld, ci-sdr, antlr4-python3-runtime, jaconv\n  Building wheel for fast-bss-eval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fast-bss-eval: filename=fast_bss_eval-0.1.3-py3-none-any.whl size=44242 sha256=5409240089aae145773471f8a34fc50943f89cee15f02b3e6b00916654ccab14\n  Stored in directory: /root/.cache/pip/wheels/d3/f4/f9/ee0192638b716fef471c69d5e5ae6e0c1f5d45dfcd53d6c3c1\n  Building wheel for ctc-segmentation (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ctc-segmentation: filename=ctc_segmentation-1.7.4-cp310-cp310-linux_x86_64.whl size=52870 sha256=88e57a9b9c70dfe91bc928a2e4090a00caa6f578ac263d3526c2d5c1188be866\n  Stored in directory: /root/.cache/pip/wheels/63/8d/64/062be0990aac6687780c9f5998af6ba110daad94354a57cea0\n  Building wheel for pyworld (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyworld: filename=pyworld-0.3.4-cp310-cp310-linux_x86_64.whl size=221147 sha256=02c69c2b2750e348ad438c038a7d48f70d537cc6fdb1f5de7f01938280549783\n  Stored in directory: /root/.cache/pip/wheels/66/09/8a/a1d79b73d59756f66e9bfe55a199840efc7473adb76ddacdfd\n  Building wheel for ci-sdr (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ci-sdr: filename=ci_sdr-0.0.2-py3-none-any.whl size=15255 sha256=0bae50aa166e54758e72efce7fbf1bf255eae85734a2f0788da6dff77ae48c4f\n  Stored in directory: /root/.cache/pip/wheels/09/e7/93/aeaa1689a57e615b04248758ae7f9af5a71a8b0b5b2dd397c8\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=e7380504d7b2a66127559a603b3a2921dc7ee38ed80998804b01ae62e96a17f6\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for jaconv (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for jaconv: filename=jaconv-0.4.0-py3-none-any.whl size=18229 sha256=e8702b1f96c523f517ec74e7dce395e9f5a97f159310bdf5f7e0e70557a291e6\n  Stored in directory: /root/.cache/pip/wheels/20/95/99/94e8d7545125181756857f6b1fc085ed4e0811ad9be7321af7\nSuccessfully built fast-bss-eval ctc-segmentation pyworld ci-sdr antlr4-python3-runtime jaconv\nInstalling collected packages: sentencepiece, jamo, jaconv, antlr4-python3-runtime, unidecode, typeguard, pypinyin, omegaconf, numpy, nltk, importlib-metadata, humanfriendly, einops, editdistance, configargparse, torch-complex, pyworld, kaldiio, hydra-core, ctc-segmentation, resampy, inflect, fast-bss-eval, ci-sdr, asteroid-filterbanks, librosa, espnet-tts-frontend, espnet\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.2.0\n    Uninstalling sentencepiece-0.2.0:\n      Successfully uninstalled sentencepiece-0.2.0\n  Attempting uninstall: typeguard\n    Found existing installation: typeguard 4.1.5\n    Uninstalling typeguard-4.1.5:\n      Successfully uninstalled typeguard-4.1.5\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 6.11.0\n    Uninstalling importlib-metadata-6.11.0:\n      Successfully uninstalled importlib-metadata-6.11.0\n  Attempting uninstall: inflect\n    Found existing installation: inflect 7.3.1\n    Uninstalling inflect-7.3.1:\n      Successfully uninstalled inflect-7.3.1\n  Attempting uninstall: librosa\n    Found existing installation: librosa 0.10.2.post1\n    Uninstalling librosa-0.10.2.post1:\n      Successfully uninstalled librosa-0.10.2.post1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nucx-py 0.38.0 requires libucx<1.16,>=1.15.0, which is not installed.\nucxx 0.38.0 requires libucx>=1.15.0, which is not installed.\nalbumentations 1.4.0 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nopentelemetry-api 1.22.0 requires importlib-metadata<7.0,>=6.0, but you have importlib-metadata 4.13.0 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\npylibraft 24.6.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nrmm 24.6.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\ntensorstore 0.1.63 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nxarray 2024.6.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\nyapf 0.40.2 requires importlib-metadata>=6.6.0, but you have importlib-metadata 4.13.0 which is incompatible.\nydata-profiling 4.6.4 requires typeguard<5,>=4.1.2, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 ci-sdr-0.0.2 configargparse-1.7 ctc-segmentation-1.7.4 editdistance-0.8.1 einops-0.8.0 espnet-202402 espnet-tts-frontend-0.0.3 fast-bss-eval-0.1.3 humanfriendly-10.0 hydra-core-1.3.2 importlib-metadata-4.13.0 inflect-7.0.0 jaconv-0.4.0 jamo-0.4.1 kaldiio-2.18.0 librosa-0.9.2 nltk-3.9.1 numpy-1.23.5 omegaconf-2.3.0 pypinyin-0.44.0 pyworld-0.3.4 resampy-0.4.3 sentencepiece-0.1.97 torch-complex-0.4.4 typeguard-2.13.3 unidecode-1.3.8\nRequirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.9.2)\nCollecting librosa\n  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\nRequirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.23.5)\nRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.11.4)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.58.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3.7)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.9.0)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.7)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.41.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (21.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pooch>=1.1->librosa) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.7.4)\nDownloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: librosa\n  Attempting uninstall: librosa\n    Found existing installation: librosa 0.9.2\n    Uninstalling librosa-0.9.2:\n      Successfully uninstalled librosa-0.9.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nespnet 202402 requires librosa==0.9.2, but you have librosa 0.10.2.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed librosa-0.10.2.post1\nCollecting indic_unified_parser\n  Downloading indic_unified_parser-1.0.6-py3-none-any.whl.metadata (3.0 kB)\nDownloading indic_unified_parser-1.0.6-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m581.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: indic_unified_parser\nSuccessfully installed indic_unified_parser-1.0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install espnet==202209","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:31:25.725670Z","iopub.execute_input":"2024-08-24T01:31:25.725998Z","iopub.status.idle":"2024-08-24T01:31:43.970147Z","shell.execute_reply.started":"2024-08-24T01:31:25.725966Z","shell.execute_reply":"2024-08-24T01:31:43.968865Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting espnet==202209\n  Downloading espnet-202209-py3-none-any.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m558.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: setuptools>=38.5.1 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (69.0.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (21.3)\nRequirement already satisfied: configargparse>=1.2.1 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (1.7)\nRequirement already satisfied: typeguard>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (2.13.3)\nRequirement already satisfied: humanfriendly in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (10.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (1.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (3.13.1)\nRequirement already satisfied: librosa>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.10.2.post1)\nRequirement already satisfied: jamo==0.4.1 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.4.1)\nRequirement already satisfied: PyYAML>=5.1.2 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (6.0.1)\nRequirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.12.1)\nRequirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (3.10.0)\nRequirement already satisfied: kaldiio>=2.17.0 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (2.18.0)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (2.1.2)\nRequirement already satisfied: torch-complex in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.4.4)\nRequirement already satisfied: nltk>=3.4.5 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (3.9.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (1.23.5)\nCollecting protobuf<=3.20.1 (from espnet==202209)\n  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.1.97)\nRequirement already satisfied: ctc-segmentation>=1.6.6 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (1.7.4)\nRequirement already satisfied: pyworld>=0.2.10 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.3.4)\nRequirement already satisfied: pypinyin<=0.44.0 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.44.0)\nRequirement already satisfied: espnet-tts-frontend in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.0.3)\nRequirement already satisfied: ci-sdr in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.0.2)\nCollecting pytorch-wpe (from espnet==202209)\n  Downloading pytorch_wpe-0.0.1-py3-none-any.whl.metadata (242 bytes)\nRequirement already satisfied: fast-bss-eval==0.1.3 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (0.1.3)\nRequirement already satisfied: importlib-metadata<5.0 in /opt/conda/lib/python3.10/site-packages (from espnet==202209) (4.13.0)\nRequirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from ctc-segmentation>=1.6.6->espnet==202209) (3.0.8)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<5.0->espnet==202209) (3.17.0)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (0.58.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (0.3.7)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (4.9.0)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->espnet==202209) (1.0.7)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.4.5->espnet==202209) (8.1.7)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.4.5->espnet==202209) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.4.5->espnet==202209) (4.66.4)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.10.2->espnet==202209) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->espnet==202209) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->espnet==202209) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->espnet==202209) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->espnet==202209) (2024.5.0)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from ci-sdr->espnet==202209) (0.8.0)\nRequirement already satisfied: unidecode>=1.0.22 in /opt/conda/lib/python3.10/site-packages (from espnet-tts-frontend->espnet==202209) (1.3.8)\nRequirement already satisfied: inflect>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from espnet-tts-frontend->espnet==202209) (7.0.0)\nRequirement already satisfied: jaconv in /opt/conda/lib/python3.10/site-packages (from espnet-tts-frontend->espnet==202209) (0.4.0)\nRequirement already satisfied: g2p-en in /opt/conda/lib/python3.10/site-packages (from espnet-tts-frontend->espnet==202209) (2.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->espnet==202209) (3.1.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.10.2->espnet==202209) (2.21)\nRequirement already satisfied: pydantic>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from inflect>=1.0.0->espnet-tts-frontend->espnet==202209) (2.5.3)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa>=0.8.0->espnet==202209) (0.41.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa>=0.8.0->espnet==202209) (3.11.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa>=0.8.0->espnet==202209) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa>=0.8.0->espnet==202209) (3.2.0)\nRequirement already satisfied: distance>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from g2p-en->espnet-tts-frontend->espnet==202209) (0.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->espnet==202209) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->espnet==202209) (1.3.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=1.0.0->espnet-tts-frontend->espnet==202209) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=1.0.0->espnet-tts-frontend->espnet==202209) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->espnet==202209) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->espnet==202209) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->espnet==202209) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->espnet==202209) (2024.7.4)\nDownloading espnet-202209-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\nInstalling collected packages: pytorch-wpe, protobuf, espnet\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: espnet\n    Found existing installation: espnet 202402\n    Uninstalling espnet-202402:\n      Successfully uninstalled espnet-202402\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-artifact-registry 1.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-dlp 3.14.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-monitoring 2.18.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-pubsub 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-resource-manager 1.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-spanner 3.40.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-videointelligence 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogleapis-common-protos 1.62.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngrpc-google-iam-v1 0.12.7 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.16.1 requires protobuf>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\ntensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\ntensorflow-serving-api 2.14.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed espnet-202209 protobuf-3.20.1 pytorch-wpe-0.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport attridict\nimport torch\nimport os\nimport argparse\nimport sys","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:31:43.971960Z","iopub.execute_input":"2024-08-24T01:31:43.972377Z","iopub.status.idle":"2024-08-24T01:31:49.816579Z","shell.execute_reply.started":"2024-08-24T01:31:43.972337Z","shell.execute_reply":"2024-08-24T01:31:49.815666Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"SAMPLING_RATE = 22050\nMAX_WAV_VALUE = 32768.0","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:31:49.818650Z","iopub.execute_input":"2024-08-24T01:31:49.819120Z","iopub.status.idle":"2024-08-24T01:31:49.823437Z","shell.execute_reply.started":"2024-08-24T01:31:49.819091Z","shell.execute_reply":"2024-08-24T01:31:49.822553Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/smtiitm/Fastspeech2_HS.git","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:31:49.824900Z","iopub.execute_input":"2024-08-24T01:31:49.825215Z","iopub.status.idle":"2024-08-24T01:32:55.503976Z","shell.execute_reply.started":"2024-08-24T01:31:49.825171Z","shell.execute_reply":"2024-08-24T01:32:55.502831Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Cloning into 'Fastspeech2_HS'...\nremote: Enumerating objects: 1258, done.\u001b[K\nremote: Counting objects: 100% (32/32), done.\u001b[K\nremote: Compressing objects: 100% (26/26), done.\u001b[K\nremote: Total 1258 (delta 10), reused 24 (delta 5), pack-reused 1226 (from 1)\u001b[K\nReceiving objects: 100% (1258/1258), 1.07 GiB | 39.13 MiB/s, done.\nResolving deltas: 100% (476/476), done.\nUpdating files: 100% (499/499), done.\nFiltering content: 100% (31/31), 4.29 GiB | 147.46 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"os.chdir('Fastspeech2_HS')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:32:55.505683Z","iopub.execute_input":"2024-08-24T01:32:55.506026Z","iopub.status.idle":"2024-08-24T01:32:55.513117Z","shell.execute_reply.started":"2024-08-24T01:32:55.505994Z","shell.execute_reply":"2024-08-24T01:32:55.510763Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"sys.path.append('hifigan')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:32:55.514685Z","iopub.execute_input":"2024-08-24T01:32:55.515166Z","iopub.status.idle":"2024-08-24T01:33:05.204664Z","shell.execute_reply.started":"2024-08-24T01:32:55.515130Z","shell.execute_reply":"2024-08-24T01:33:05.203389Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from models import Generator","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:33:05.208093Z","iopub.execute_input":"2024-08-24T01:33:05.209065Z","iopub.status.idle":"2024-08-24T01:33:05.232597Z","shell.execute_reply.started":"2024-08-24T01:33:05.209021Z","shell.execute_reply":"2024-08-24T01:33:05.230911Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from espnet2.bin.tts_inference import Text2Speech","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:33:05.234186Z","iopub.execute_input":"2024-08-24T01:33:05.234661Z","iopub.status.idle":"2024-08-24T01:33:10.814136Z","shell.execute_reply.started":"2024-08-24T01:33:05.234620Z","shell.execute_reply":"2024-08-24T01:33:10.812987Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def load_hifigan_vocoder(language, gender, device):\n    # Load HiFi-GAN vocoder configuration file and generator model for the specified language and gender\n    vocoder_config = f\"vocoder/{gender}/aryan/hifigan/config.json\"\n    vocoder_generator = f\"vocoder/{gender}/aryan/hifigan/generator\"\n    with open(vocoder_config,'r') as f:\n        data=f.read()\n    json_config = json.loads(data)\n    h=attridict(json_config)\n    device=torch.device(device)\n    generator=Generator(h).to(device)\n    state_dict_g=torch.load(vocoder_generator,device)\n    generator.load_state_dict(state_dict_g['generator'])\n    generator.eval()\n    generator.remove_weight_norm()\n    \n    return generator\n\ndef load_fastspeech2model(language,gender,device):\n    with open(f\"{language}/{gender}/model/config.yaml\",'r') as file:\n        config=yaml.safe_load(file)\n        \n    current_working_directory=os.getcwd()\n\n    feat=\"model/feats_stats.npz\"\n    pitch=\"model/pitch_stats.npz\"\n    energy=\"model/energy_stats.npz\"\n\n    feat_path=os.path.join(current_working_directory,language,gender,feat)\n    pitch_path=os.path.join(current_working_directory,language,gender,pitch)\n    energy_path=os.path.join(current_working_directory,language,gender,energy)\n\n    config[\"normalize_conf\"][\"stats_file\"]=feat_path\n    config[\"pitch_normalize_conf\"][\"stats_file\"]=pitch_path\n    config[\"energy_normalize_conf\"][\"stats_file\"]=energy_path\n        \n    with open(f\"{language}/{gender}/model/config.yaml\",'w') as file:\n        yaml.dump(config,file)\n    \n    tts_model=f\"{language}/{gender}/model/model.pth\"\n    tts_config=f\"{language}/{gender}/model/config.yaml\"\n    \n    return Text2Speech(train_config=tts_config,model_file=tts_model,device=device)\n\ndef text_synthesis(language,gender,sample_text,vocoder,MAX_WAV_VALUE,device,alpha):\n    with torch.no_grad():\n        model=load_fastspeech2model(language,gender,device)\n        \n        out=model(sample_text,decode_conf={'alpha':alpha})\n        print(\"TTS done\")\n        \n        x=out['feat_gen_denorm'].T.unsqueeze(0)*2.3262\n        x=x.to(device)\n        \n        y_g_hat=vocoder(x)\n        audio=y_g_hat.squeeze()\n        audio=audio*MAX_WAV_VALUE\n        audio=audio.cpu().numpy().astype('int16')\n        \n        return audio\n\ndef split_into_chunks(text,words_per_chunk=100):\n    words=text.split()\n    chunks=[words[i:i+words_per_chunk] for i in range(0,len(words),words_per_chunk)]\n    return [' '.join(chunk) for chunk in chunks]\n    \n        \n            \n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:33:10.817988Z","iopub.execute_input":"2024-08-24T01:33:10.818568Z","iopub.status.idle":"2024-08-24T01:33:10.835663Z","shell.execute_reply.started":"2024-08-24T01:33:10.818538Z","shell.execute_reply":"2024-08-24T01:33:10.834454Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%writefile text_preprocess_for_inference.py\n'''\nTTS Preprocessing\nDeveloped by Arun Kumar A(CS20S013) - November 2022\nCode Changes by Utkarsh - 2023\n'''\nimport os\nimport re\nimport json\nimport pandas as pd\nimport string\nfrom collections import defaultdict\nimport time\nimport subprocess\nimport shutil\nfrom multiprocessing import Process\nimport traceback\n\n#imports of dependencies from environment.yml\nfrom indic_numtowords import num2words\nfrom g2p_en import G2p\n\ndef add_to_dictionary(dict_to_add, dict_file):\n    append_string = \"\"\n    for key, value in dict_to_add.items():\n        append_string += (str(key) + \" \" + str(value) + \"\\n\")\n    \n    if os.path.isfile(dict_file):\n        # make a copy of the dictionary\n        source_dir = os.path.dirname(dict_file)\n        dict_file_name = os.path.basename(dict_file)\n        temp_file_name = \".\" + dict_file_name + \".temp\"\n        temp_dict_file = os.path.join(source_dir, temp_file_name)\n        shutil.copy(dict_file, temp_dict_file)\n        # append the new words in the dictionary to the temp file\n        with open(temp_dict_file, \"a\") as f:\n            f.write(append_string)\n        # check if the write is successful and then replace the temp file as the dict file\n        try:\n            df_orig = pd.read_csv(dict_file, delimiter=\" \", header=None, dtype=str)\n            df_temp = pd.read_csv(temp_dict_file, delimiter=\" \", header=None, dtype=str)\n            if len(df_temp) > len(df_orig):\n                os.rename(temp_dict_file, dict_file)\n                print(f\"{len(dict_to_add)} new words appended to Dictionary: {dict_file}\")\n        except:\n            print(traceback.format_exc())\n    else:\n        # create a new dictionary\n        with open(dict_file, \"a\") as f:\n            f.write(append_string)\n        print(f\"New Dictionary: {dict_file} created with {len(dict_to_add)} words\")\n\n\nclass TextCleaner:\n    def __init__(self):\n        # this is a static set of cleaning rules to be applied\n        self.cleaning_rules = {\n            \" +\" : \" \",\n            \"^ +\" : \"\",\n            \" +$\" : \"\",\n            \"#\" : \"\",\n            \"[.,;।!](\\r\\n)*\" : \"# \",\n            \"[.,;।!](\\n)*\" : \"# \",\n            \"(\\r\\n)+\" : \"# \",\n            \"(\\n)+\" : \"# \",\n            \"(\\r)+\" : \"# \",\n            \"\"\"[?;:)(!|&’‘,।\\.\"]\"\"\": \"\",\n            \"[/']\" : \"\",\n            \"[-–]\" : \" \",\n        }\n\n    def clean(self, text):\n        for key, replacement in self.cleaning_rules.items():\n            text = re.sub(key, replacement, text)\n        return text\n\n    def clean_list(self, text):\n        # input is supposed to be a list of strings\n        output_text = []\n        for line in text:\n            line = line.strip()\n            for key, replacement in self.cleaning_rules.items():\n                line = re.sub(key, replacement, line)\n            output_text.append(line)\n        return output_text\n\n\nclass Phonifier:\n    def __init__(self, dict_location=None):\n        if dict_location is None:\n            dict_location = \"phone_dict\"\n        self.dict_location = dict_location\n\n        # self.phone_dictionary = {}\n        # # load dictionary for all the available languages\n        # for dict_file in os.listdir(dict_location):\n        #     try:\n        #         if dict_file.startswith(\".\"):\n        #             # ignore hidden files\n        #             continue\n        #         language = dict_file\n        #         dict_file_path = os.path.join(dict_location, dict_file)\n        #         df = pd.read_csv(dict_file_path, delimiter=\" \", header=None, dtype=str)\n        #         self.phone_dictionary[language] = df.set_index(0).to_dict('dict')[1]\n        #     except Exception as e:\n        #         print(traceback.format_exc())\n\n        # print(\"Phone dictionary loaded for the following languages:\", list(self.phone_dictionary.keys()))\n\n        self.g2p = G2p()\n        print('Loading G2P model... Done!')\n        # Mapping between the cmu phones and the iitm cls\n        self.cmu_2_cls_map = {\n            \"AA\" : \"aa\",\n            \"AA0\" : \"aa\",\n            \"AA1\" : \"aa\",\n            \"AA2\" : \"aa\",\n            \"AE\" : \"axx\",\n            \"AE0\" : \"axx\",\n            \"AE1\" : \"axx\",\n            \"AE2\" : \"axx\",\n            \"AH\" : \"a\",\n            \"AH0\" : \"a\",\n            \"AH1\" : \"a\",\n            \"AH2\" : \"a\",\n            \"AO\" : \"ax\",\n            \"AO0\" : \"ax\",\n            \"AO1\" : \"ax\",\n            \"AO2\" : \"ax\",\n            \"AW\" : \"ou\",\n            \"AW0\" : \"ou\",\n            \"AW1\" : \"ou\",\n            \"AW2\" : \"ou\",\n            \"AX\" : \"a\",\n            \"AY\" : \"ei\",\n            \"AY0\" : \"ei\",\n            \"AY1\" : \"ei\",\n            \"AY2\" : \"ei\",\n            \"B\" : \"b\",\n            \"CH\" : \"c\",\n            \"D\" : \"dx\",\n            \"DH\" : \"d\",\n            \"EH\" : \"ee\",\n            \"EH0\" : \"ee\",\n            \"EH1\" : \"ee\",\n            \"EH2\" : \"ee\",\n            \"ER\" : \"a r\",\n            \"ER0\" : \"a r\",\n            \"ER1\" : \"a r\",\n            \"ER2\" : \"a r\",\n            \"EY\" : \"ee\",\n            \"EY0\" : \"ee\",\n            \"EY1\" : \"ee\",\n            \"EY2\" : \"ee\",\n            \"F\" : \"f\",\n            \"G\" : \"g\",\n            \"HH\" : \"h\",\n            \"IH\" : \"i\",\n            \"IH0\" : \"i\",\n            \"IH1\" : \"i\",\n            \"IH2\" : \"i\",\n            \"IY\" : \"ii\",\n            \"IY0\" : \"ii\",\n            \"IY1\" : \"ii\",\n            \"IY2\" : \"ii\",\n            \"JH\" : \"j\",\n            \"K\" : \"k\",\n            \"L\" : \"l\",\n            \"M\" : \"m\",\n            \"N\" : \"n\",\n            \"NG\" : \"ng\",\n            \"OW\" : \"o\",\n            \"OW0\" : \"o\",\n            \"OW1\" : \"o\",\n            \"OW2\" : \"o\",\n            \"OY\" : \"ei\",\n            \"OY0\" : \"ei\",\n            \"OY1\" : \"ei\",\n            \"OY2\" : \"ei\",\n            \"P\" : \"p\",\n            \"R\" : \"r\",\n            \"S\" : \"s\",\n            \"SH\" : \"sh\",\n            \"T\" : \"tx\",\n            \"TH\" : \"t\",\n            \"UH\" : \"u\",\n            \"UH0\" : \"u\",\n            \"UH1\" : \"u\",\n            \"UH2\" : \"u\",\n            \"UW\" : \"uu\",\n            \"UW0\" : \"uu\",\n            \"UW1\" : \"uu\",\n            \"UW2\" : \"uu\",\n            \"V\" : \"w\",\n            \"W\" : \"w\",\n            \"Y\" : \"y\",\n            \"Z\" : \"z\",\n            \"ZH\" : \"sh\",\n        }\n\n        # Mapping between the iitm cls and iitm char\n        self.cls_2_chr_map = {\n            \"aa\" : \"A\",\n            \"ii\" : \"I\",\n            \"uu\" : \"U\",\n            \"ee\" : \"E\",\n            \"oo\" : \"O\",\n            \"nn\" : \"N\",\n            \"ae\" : \"ऍ\",\n            \"ag\" : \"ऽ\",\n            \"au\" : \"औ\",\n            \"axx\" : \"अ\",\n            \"ax\" : \"ऑ\",\n            \"bh\" : \"B\",\n            \"ch\" : \"C\",\n            \"dh\" : \"ध\",\n            \"dx\" : \"ड\",\n            \"dxh\" : \"ढ\",\n            \"dxhq\" : \"T\",\n            \"dxq\" : \"D\",\n            \"ei\" : \"ऐ\",\n            \"ai\" : \"ऐ\",\n            \"eu\" : \"உ\",\n            \"gh\" : \"घ\",\n            \"gq\" : \"G\",\n            \"hq\" : \"H\",\n            \"jh\" : \"J\",\n            \"kh\" : \"ख\",\n            \"khq\" : \"K\",\n            \"kq\" : \"क\",\n            \"ln\" : \"ൾ\",\n            \"lw\" : \"ൽ\",\n            \"lx\" : \"ള\",\n            \"mq\" : \"M\",\n            \"nd\" : \"न\",\n            \"ng\" : \"ङ\",\n            \"nj\" : \"ञ\",\n            \"nk\" : \"Y\",\n            \"nw\" : \"ൺ\",\n            \"nx\" : \"ण\",\n            \"ou\" : \"औ\",\n            \"ph\" : \"P\",\n            \"rq\" : \"R\",\n            \"rqw\" : \"ॠ\",\n            \"rw\" : \"ർ\",\n            \"rx\" : \"र\",\n            \"sh\" : \"श\",\n            \"sx\" : \"ष\",\n            \"th\" : \"थ\",\n            \"tx\" : \"ट\",\n            \"txh\" : \"ठ\",\n            \"wv\" : \"W\",\n            \"zh\" : \"Z\",\n        }\n\n        # Multilingual support for OOV characters\n        oov_map_json_file = 'multilingualcharmap.json'\n        with open(oov_map_json_file, 'r') as oov_file:\n            self.oov_map = json.load(oov_file)\n\n\n\n    def load_lang_dict(self, language, phone_dictionary):            \n        # load dictionary for requested language\n        try:\n\n            dict_file = language\n            print(\"language\", language)\n            dict_file_path = os.path.join(self.dict_location, dict_file)\n            print(\"dict_file_path\", dict_file_path)\n            df = pd.read_csv(dict_file_path, delimiter=\" \", header=None, dtype=str)\n            phone_dictionary[language] = df.set_index(0).to_dict('dict')[1]\n\n            dict_file = 'english'\n            dict_file_path = os.path.join(self.dict_location, dict_file)\n            df = pd.read_csv(dict_file_path, delimiter=\" \", header=None, dtype=str)\n            phone_dictionary['english'] = df.set_index(0).to_dict('dict')[1]\n            \n        except Exception as e:\n            print(traceback.format_exc())\n\n        return phone_dictionary\n\n    def __is_float(self, word):\n        parts = word.split('.')\n        if len(parts) != 2:\n            return False\n        return parts[0].isdecimal() and parts[1].isdecimal()\n\n    def en_g2p(self, word):\n        phn_out = self.g2p(word)\n        # print(f\"phn_out: {phn_out}\")\n        # iterate over the string list and replace each word with the corresponding value from the dictionary\n        for i, phn in enumerate(phn_out):\n            if phn in self.cmu_2_cls_map.keys():\n                phn_out[i] = self.cmu_2_cls_map[phn]\n                # cls_out = self.cmu_2_cls_map[phn]\n                if phn_out[i] in self.cls_2_chr_map.keys():\n                    phn_out[i] = self.cls_2_chr_map[phn_out[i]]\n                else:\n                    pass\n            else:\n                pass  # ignore words that are not in the dictionary\n            # print(f\"i: {i}, phn: {phn}, cls_out: {cls_out}, phn_out: {phn_out[i]}\")\n        return (\"\".join(phn_out)).strip().replace(\" \", \"\")\n\n    def __post_phonify(self, text, language, gender):\n        language_gender_id = language+'_'+gender\n        if language_gender_id in self.oov_map.keys():\n            output_string = ''\n            for char in text:\n                if char in self.oov_map[language_gender_id].keys():\n                    output_string += self.oov_map[language_gender_id][char]\n                else:\n                    output_string += char\n                # output_string += self.oov_map['language_gender_id']['char']\n            return output_string\n        else:\n            return text\n\n    def __is_english_word(self, word):\n        maxchar = max(word)\n        if u'\\u0000' <= maxchar <= u'\\u007f':\n            return True\n        return False\n\n    def __phonify(self, text, language, gender, phone_dictionary):\n        # text is expected to be a list of strings\n        words = set((\" \".join(text)).split(\" \"))\n        #print(f\"words test: {words}\")\n        non_dict_words = []\n       \n        \n        if language in phone_dictionary:\n            for word in words:\n                # print(f\"word: {word}\")\n                if word not in phone_dictionary[language] and (language == \"english\" or (not self.__is_english_word(word))):\n                    non_dict_words.append(word)\n                    #print('INSIDE IF CONDITION OF ADDING WORDS')\n        else:\n            non_dict_words = words\n        print(f\"word not in dict: {non_dict_words}\")\n\n        if len(non_dict_words) > 0:\n            # unified parser has to be run for the non dictionary words\n            os.makedirs(\"tmp\", exist_ok=True)\n            timestamp = str(time.time())\n            non_dict_words_file = os.path.abspath(\"tmp/non_dict_words_\" + timestamp)\n            out_dict_file = os.path.abspath(\"tmp/out_dict_\" + timestamp)\n            with open(non_dict_words_file, \"w\") as f:\n                f.write(\"\\n\".join(non_dict_words))\n\n            if(language == 'tamil'):\n                current_directory = os.getcwd()\n                #tamil_parser_cmd = \"tamil_parser.sh\"\n                tamil_parser_cmd = f\"{current_directory}/ssn_parser_new/tamil_parser.py\"\n                #subprocess.run([\"bash\", tamil_parser_cmd, non_dict_words_file, out_dict_file, timestamp, \"ssn_parser\"])\n                subprocess.run([\"python\", tamil_parser_cmd, non_dict_words_file, out_dict_file, timestamp, f\"{current_directory}/ssn_parser_new\"])\n            elif(language == 'english'):\n                phn_out_dict = {}\n                for i in range(0,len(non_dict_words)):\n                    phn_out_dict[non_dict_words[i]] = self.en_g2p(non_dict_words[i])\n                # Create a string representation of the dictionary\n                data_str = \"\\n\".join([f\"{key}\\t{value}\" for key, value in phn_out_dict.items()])\n                print(f\"data_str: {data_str}\")\n                with open(out_dict_file, \"w\") as f:\n                    f.write(data_str)\n            else:\n          \n                out_dict_file = os.path.abspath(\"tmp/out_dict_\" + timestamp)\n                from get_phone_mapped_python import TextReplacer\n                \n                from indic_unified_parser.uparser import wordparse\n                \n                text_replacer=TextReplacer()\n                # def write_output_to_file(output_text, file_path):\n                #     with open(file_path, 'w') as f:\n                #         f.write(output_text)\n                parsed_output_list = []\n                for word in non_dict_words:\n                    parsed_word = wordparse(word, 0, 0, 1)\n                    parsed_output_list.append(parsed_word)\n                replaced_output_list = [text_replacer.apply_replacements(parsed_word) for parsed_word in parsed_output_list]\n                with open(out_dict_file, 'w', encoding='utf-8') as file:\n                    for original_word, formatted_word in zip(non_dict_words, replaced_output_list):\n                        line = f\"{original_word}\\t{formatted_word}\\n\"\n                        file.write(line)\n                        print(line, end='') \n                  \n\n            try:\n                \n                df = pd.read_csv(out_dict_file, delimiter=\"\\t\", header=None, dtype=str)\n                #print('DATAFRAME OUTPUT FILE', df.head())\n                new_dict = df.dropna().set_index(0).to_dict('dict')[1]\n                #print(\"new dict\",new_dict)\n                if language not in phone_dictionary:\n                    phone_dictionary[language] = new_dict\n                else:\n                    phone_dictionary[language].update(new_dict)\n                # run a non-blocking child process to update the dictionary file\n                #print(\"phone_dict\", self.phone_dictionary)\n                p = Process(target=add_to_dictionary, args=(new_dict, os.path.join(self.dict_location, language)))\n                p.start()\n            except Exception as err:\n                print(f\"Error: While loading {out_dict_file}\")\n                traceback.print_exc()\n\n        # phonify text with dictionary\n        text_phonified = []\n        for phrase in text:\n            phrase_phonified = []\n            for word in phrase.split(\" \"):\n                if self.__is_english_word(word):\n                    if word in phone_dictionary[\"english\"]:\n                        phrase_phonified.append(str(phone_dictionary[\"english\"][word]))\n                    else:\n                        phrase_phonified.append(str(self.en_g2p(word)))\n                elif word in phone_dictionary[language]:\n                    # if a word could not be parsed, skip it\n                    phrase_phonified.append(str(phone_dictionary[language][word]))\n            # text_phonified.append(self.__post_phonify(\" \".join(phrase_phonified),language, gender))\n            text_phonified.append(\" \".join(phrase_phonified))\n        return text_phonified\n\n    def __merge_lists(self, lists):\n        merged_string = \"\"\n        for list in lists:\n            for word in list:\n                merged_string += word + \" \"\n        return merged_string.strip()\n\n    def __phonify_list(self, text, language, gender, phone_dictionary):\n        # text is expected to be a list of list of strings\n        words = set(self.__merge_lists(text).split(\" \"))\n        non_dict_words = []\n        if language in phone_dictionary:\n            for word in words:\n                if word not in phone_dictionary[language] and (language == \"english\" or (not self.__is_english_word(word))):\n                    non_dict_words.append(word)\n        else:\n            non_dict_words = words\n\n        if len(non_dict_words) > 0:\n            print(len(non_dict_words))\n            print(non_dict_words)\n            # unified parser has to be run for the non dictionary words\n            os.makedirs(\"tmp\", exist_ok=True)\n            timestamp = str(time.time())\n            non_dict_words_file = os.path.abspath(\"tmp/non_dict_words_\" + timestamp)\n            out_dict_file = os.path.abspath(\"tmp/out_dict_\" + timestamp)\n            with open(non_dict_words_file, \"w\") as f:\n                f.write(\"\\n\".join(non_dict_words))\n\n            if(language == 'tamil'):\n                current_directory = os.getcwd()\n                #tamil_parser_cmd = \"tamil_parser.sh\"\n                tamil_parser_cmd = f\"{current_directory}/ssn_parser_new/tamil_parser.py\"\n                #subprocess.run([\"bash\", tamil_parser_cmd, non_dict_words_file, out_dict_file, timestamp, \"ssn_parser\"])\n                subprocess.run([\"python\", tamil_parser_cmd, non_dict_words_file, out_dict_file, timestamp, f\"{current_directory}/ssn_parser_new\"])\n                \n            elif(language == 'english'):\n                phn_out_dict = {}\n                for i in range(0,len(non_dict_words)):\n                    phn_out_dict[non_dict_words[i]] = self.en_g2p(non_dict_words[i])\n                # Create a string representation of the dictionary\n                data_str = \"\\n\".join([f\"{key}\\t{value}\" for key, value in phn_out_dict.items()])\n                print(f\"data_str: {data_str}\")\n                with open(out_dict_file, \"w\") as f:\n                    f.write(data_str)\n            else:\n                out_dict_file = os.path.abspath(\"tmp/out_dict_\" + timestamp)\n                from get_phone_mapped_python import TextReplacer\n                \n                from indic_unified_parser.uparser import wordparse\n                \n                text_replacer=TextReplacer()\n            \n                parsed_output_list = []\n                for word in non_dict_words:\n                    parsed_word = wordparse(word, 0, 0, 1)\n                    parsed_output_list.append(parsed_word)\n                replaced_output_list = [text_replacer.apply_replacements(parsed_word) for parsed_word in parsed_output_list]\n                with open(out_dict_file, 'w', encoding='utf-8') as file:\n                    for original_word, formatted_word in zip(non_dict_words, replaced_output_list):\n                        line = f\"{original_word}\\t{formatted_word}\\n\"\n                        file.write(line)\n                        print(line, end='') \n        \n            try:\n                df = pd.read_csv(out_dict_file, delimiter=\"\\t\", header=None, dtype=str)\n                new_dict = df.dropna().set_index(0).to_dict('dict')[1]\n                print(new_dict)\n                if language not in phone_dictionary:\n                    phone_dictionary[language] = new_dict\n                else:\n                    phone_dictionary[language].update(new_dict)\n                # run a non-blocking child process to update the dictionary file\n                p = Process(target=add_to_dictionary, args=(new_dict, os.path.join(self.dict_location, language)))\n                p.start()\n            except Exception as err:\n                traceback.print_exc()\n\n        # phonify text with dictionary\n        text_phonified = []\n        for line in text:\n            line_phonified = []\n            for phrase in line:\n                phrase_phonified = []\n                for word in phrase.split(\" \"):\n                    if self.__is_english_word(word):\n                        if word in phone_dictionary[\"english\"]:\n                            phrase_phonified.append(str(phone_dictionary[\"english\"][word]))\n                        else:\n                            phrase_phonified.append(str(self.en_g2p(word)))\n                    elif word in phone_dictionary[language]:\n                        # if a word could not be parsed, skip it\n                        phrase_phonified.append(str(phone_dictionary[language][word]))\n                # line_phonified.append(self.__post_phonify(\" \".join(phrase_phonified), language, gender))\n                line_phonified.append(\" \".join(phrase_phonified))\n            text_phonified.append(line_phonified)\n        return text_phonified\n\n    def phonify(self, text, language, gender, phone_dictionary):\n        if not isinstance(text, list):\n            out = self.__phonify([text], language, gender)\n            return out[0]\n        return self.__phonify(text, language, gender, phone_dictionary)\n    \n    def phonify_list(self, text, language, gender, phone_dictionary):\n        if isinstance(text, list):\n            return self.__phonify_list(text, language, gender, phone_dictionary)\n        else:\n            print(\"Error!! Expected to have a list as input.\")\n\n\nclass TextNormalizer:\n    def __init__(self, char_map_location=None):\n        # self.phonifier = phonifier\n        if char_map_location is None:\n            char_map_location = \"charmap\"\n    \n        # this is a static set of cleaning rules to be applied\n        self.cleaning_rules = {\n            \" +\" : \" \",\n            \"^ +\" : \"\",\n            \" +$\" : \"\",\n            \"#$\" : \"\",\n            \"# +$\" : \"\",\n        }\n\n        # this is the list of languages supported by num_to_words\n        self.keydict = {\"english\" : \"en\",\n            \"hindi\" : \"hi\",\n            \"gujarati\" : \"gu\",\n            \"marathi\" : \"mr\",\n            \"bengali\" : \"bn\",\n            \"telugu\" : \"te\",\n            \"tamil\" : \"ta\",\n            \"kannada\" : \"kn\",\n            \"odia\" : \"or\",\n            \"punjabi\" : \"pa\"\n        }\n        \n        # self.g2p = G2p()\n        # print('Loading G2P model... Done!')\n\n    def __post_cleaning(self, text):\n        for key, replacement in self.cleaning_rules.items():\n            text = re.sub(key, replacement, text)\n        return text\n\n    def __post_cleaning_list(self, text):\n        # input is supposed to be a list of strings\n        output_text = []\n        for line in text:\n            for key, replacement in self.cleaning_rules.items():\n                line = re.sub(key, replacement, line)\n            output_text.append(line)\n        return output_text\n\n    def __check_char_type(self, str_c):\n        # Determine the type of the character\n        if str_c.isnumeric():\n            char_type = \"number\"\n        elif str_c in string.punctuation:\n            char_type = \"punctuation\"\n        elif str_c in string.whitespace:\n            char_type = \"whitespace\"\n        elif str_c.isalpha() and str_c.isascii():\n            char_type = \"ascii\"\n        else:\n            char_type = \"non-ascii\"\n        return char_type\n    \n    def insert_space(self, text):\n        '''\n        Check if the text contains numbers and English words and if they are without space inserts space between them.\n        '''\n        # Initialize variables to track the previous character type and whether a space should be inserted\n        prev_char_type = None\n        next_char_type = None\n        insert_space = False\n\n        # Output string\n        output_string = \"\"\n\n        # Iterate through each character in the text\n        for i, c in enumerate(text):\n            # Determine the type of the character\n            char_type = self.__check_char_type(c)\n            if i == (len(text) - 1):\n                next_char_type = None\n            else:\n                next_char_type = self.__check_char_type(text[i+1])\n            # print(f\"{i}: {c} is a {char_type} character and next character is a {next_char_type}\")\n\n            # If the character type has changed from the previous character, check if a space should be inserted\n            if (char_type != prev_char_type and prev_char_type != None and char_type != \"punctuation\" and char_type != \"whitespace\"):\n                if next_char_type != \"punctuation\" or next_char_type != \"whitespace\":\n                    insert_space = True\n\n            # Insert a space if needed\n            if insert_space:\n                output_string += \" \"+c\n                insert_space = False\n            else:\n                output_string += c\n\n            # Update the previous character type\n            prev_char_type = char_type\n\n        # Print the modified text\n        output_string = re.sub(r' +', ' ', output_string)\n        return output_string\n\n    def insert_space_list(self, text):\n        '''\n        Expect the input to be in form of list of string.\n        Check if the text contains numbers and English words and if they are without space inserts space between them.\n        '''\n        # Output string list\n        output_list = []\n\n        for line in text:\n            # Initialize variables to track the previous character type and whether a space should be inserted\n            prev_char_type = None\n            next_char_type = None\n            insert_space = False\n            # Output string\n            output_string = \"\"\n            # Iterate through each character in the line\n            for i, c in enumerate(line):\n                # Determine the type of the character\n                char_type = self.__check_char_type(c)\n                if i == (len(line) - 1):\n                    next_char_type = None\n                else:\n                    next_char_type = self.__check_char_type(line[i+1])\n                # print(f\"{i}: {c} is a {char_type} character and next character is a {next_char_type}\")\n\n                # If the character type has changed from the previous character, check if a space should be inserted\n                if (char_type != prev_char_type and prev_char_type != None and char_type != \"punctuation\" and char_type != \"whitespace\"):\n                    if next_char_type != \"punctuation\" or next_char_type != \"whitespace\":\n                        insert_space = True\n\n                # Insert a space if needed\n                if insert_space:\n                    output_string += \" \"+c\n                    insert_space = False\n                else:\n                    output_string += c\n\n                # Update the previous character type\n                prev_char_type = char_type\n\n            # Print the modified line\n            output_string = re.sub(r' +', ' ', output_string)\n            output_list.append(output_string)\n        return output_list\n\n    def num2text(self, text, language):\n        if language in self.keydict.keys():\n            digits = sorted(list(map(int, re.findall(r'\\d+', text))),reverse=True)\n            if digits:\n                for digit in digits:\n                    text = re.sub(str(digit), ' '+num2words(digit, self.keydict[language])+' ', text)\n            return self.__post_cleaning(text)\n        else:\n            print(f\"No num-to-char for the given language {language}.\")\n            return self.__post_cleaning(text)\n\n    def num2text_list(self, text, language):\n        # input is supposed to be a list of strings\n        if language in self.keydict.keys():\n            output_text = []\n            for line in text:\n                digits = sorted(list(map(int, re.findall(r'\\d+', line))),reverse=True)\n                if digits:\n                    for digit in digits:\n                        line = re.sub(str(digit), ' '+num2words(digit, self.keydict[language])+' ', line)\n                output_text.append(line)\n            return self.__post_cleaning_list(output_text)\n        else:\n            print(f\"No num-to-char for the given language {language}.\")\n            return self.__post_cleaning_list(text)\n        \n    def numberToTextConverter(self, text, language):\n        if language in self.keydict.keys():\n            matches = re.findall(r'\\d+\\.\\d+|\\d+', text)\n            digits = sorted([int(match) if match.isdigit() else match if re.match(r'^\\d+(\\.\\d+)?$', match) else str(match) for match in matches], key=lambda x: float(x) if isinstance(x, str) and '.' in x else x, reverse=True)\n            if digits:\n                for digit in digits:\n                        \n                    if isinstance(digit, int):\n                        text = re.sub(str(digit), ' '+num2words(digit, self.keydict[language]).replace(\",\", \"\")+' ', text)\n                    else:\n                        parts = str(digit).split('.')\n                        integer_part = int(parts[0])\n                        data1 = num2words(integer_part, self.keydict[language]).replace(\",\", \"\")\n                        decimal_part = str(parts[1])\n                        data2 = ''\n                        for i in decimal_part:\n                            data2 = data2+' '+num2words(i, self.keydict[language])\n                        if language == 'hindi':\n                            final_data = f'{data1} दशमलव {data2}'\n                        elif language == 'tamil':\n                            final_data = f'{data1} புள்ளி {data2}'\n                        else:\n                            final_data = f'{data1} point {data2}'\n\n                            \n                        text = re.sub(str(digit), ' '+final_data+' ', text)\n\n            return self.__post_cleaning(text)\n        else:\n\n\n            words = {\n                '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n                '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n            }\n\n\n            # Use regular expression to find and replace decimal points in numbers\n            text = re.sub(r'(?<=\\d)\\.(?=\\d)', ' point ', text)\n\n            # Find all occurrences of numbers with decimal points and convert them to words\n            matches = re.findall(r'point (\\d+)', text)\n\n            for match in matches:\n                replacement = ' '.join(words[digit] for digit in match)\n                text = text.replace(f'point {match}', f'point {replacement}', 1)\n\n\n            return self.__post_cleaning(text)\n\n\n    def normalize(self, text, language):\n        return self.__post_cleaning(text)\n\n    def normalize_list(self, text, language):\n        # input is supposed to be a list of strings\n        return self.__post_cleaning_list(text)\n\n\nclass TextPhrasifier:\n    @classmethod\n    def phrasify(cls, text):\n        phrase_list = []\n        for phrase in text.split(\"#\"):\n            phrase = phrase.strip()\n            if phrase != \"\":\n                phrase_list.append(phrase)\n        return phrase_list\n\nclass TextPhrasifier_List:\n    @classmethod\n    def phrasify(cls, text):\n        # input is supposed to be a list of strings\n        # output is list of list of strings\n        output_list = []\n        for line in text:\n            phrase_list = []\n            for phrase in line.split(\"#\"):\n                phrase = phrase.strip()\n                if phrase != \"\":\n                    phrase_list.append(phrase)\n            output_list.append(phrase_list)\n        return output_list\n\nclass DurAlignTextProcessor:\n    def __init__(self):\n        # this is a static set of cleaning rules to be applied\n        self.cleaning_rules = {\n            \" +\" : \"\",\n           \"^\" : \"$\",\n            \"$\" : \".\",\n        }\n        self.cleaning_rules_English = {\n            \" +\" : \"\",\n            \"$\" : \".\",\n        }\n    def textProcesor(self, text):\n        for key, replacement in self.cleaning_rules.items():\n            for idx in range(0,len(text)):\n                text[idx] = re.sub(key, replacement, text[idx])\n\n        return text\n    \n    def textProcesorForEnglish(self, text):\n        for key, replacement in self.cleaning_rules_English.items():\n            for idx in range(0,len(text)):\n                text[idx] = re.sub(key, replacement, text[idx])\n\n        return text\n    \n    def textProcesor_list(self, text):\n        # input expected in 'list of list of string' format\n        output_text = []\n        for line in text:\n            for key, replacement in self.cleaning_rules.items():\n                for idx in range(0,len(line)):\n                    line[idx] = re.sub(key, replacement, line[idx])\n            output_text.append(line)\n\n        return output_text\n\n\n\n\nclass SharedInit:\n    def __init__(self,\n                text_cleaner = TextCleaner(),\n                text_normalizer=TextNormalizer(),\n                phonifier = Phonifier(),\n                text_phrasefier = TextPhrasifier(),\n                post_processor = DurAlignTextProcessor()):\n        self.text_cleaner = text_cleaner\n        self.text_normalizer = text_normalizer\n        self.phonifier = phonifier\n        self.text_phrasefier = text_phrasefier\n        self.post_processor = post_processor\n\n\n\nclass TTSDurAlignPreprocessor(SharedInit):\n    # def __init__(self,\n    #             text_cleaner = TextCleaner(),\n    #             text_normalizer=TextNormalizer(),\n    #             phonifier = Phonifier(),\n    #             post_processor = DurAlignTextProcessor()):\n    #     self.text_cleaner = text_cleaner\n    #     self.text_normalizer = text_normalizer\n    #     self.phonifier = phonifier\n    #     self.post_processor = post_processor\n\n    def preprocess(self, text, language, gender, phone_dictionary):\n        # text = text.strip()\n        #print(text)\n        text = self.text_normalizer.numberToTextConverter(text, language)\n        text = self.text_cleaner.clean(text)\n        #print(\"cleaned text\", text)\n        # text = self.text_normalizer.insert_space(text)\n        #text = self.text_normalizer.num2text(text, language)\n        # print(text)\n        text = self.text_normalizer.normalize(text, language)\n        # print(text)\n        phrasified_text = TextPhrasifier.phrasify(text)\n        #print(\"phrased\",phrasified_text)\n\n        if language not in list(phone_dictionary.keys()):\n            phone_dictionary = self.phonifier.load_lang_dict(language, phone_dictionary)\n\n        #print(phone_dictionary.keys())\n\n        phonified_text = self.phonifier.phonify(phrasified_text, language, gender, phone_dictionary)\n        #print(\"phonetext\",phonified_text)\n        phonified_text = self.post_processor.textProcesor(phonified_text)\n        #print(phonified_text)\n        return phonified_text, phrasified_text\n\nclass TTSDurAlignPreprocessor_VTT(SharedInit):\n    # def __init__(self,\n    #             text_cleaner = TextCleaner(),\n    #             text_normalizer=TextNormalizer(),\n    #             phonifier = Phonifier(),\n    #             post_processor = DurAlignTextProcessor()):\n    #     self.text_cleaner = text_cleaner\n    #     self.text_normalizer = text_normalizer\n    #     self.phonifier = phonifier\n    #     self.post_processor = post_processor\n\n    def preprocess(self, text, language, gender):\n        # text = text.strip()\n        text = self.text_cleaner.clean_list(text)\n        # text = self.text_normalizer.insert_space_list(text)\n        text = self.text_normalizer.num2text_list(text, language)\n        text = self.text_normalizer.normalize_list(text, language)\n        phrasified_text = TextPhrasifier_List.phrasify(text)\n        phonified_text = self.phonifier.phonify_list(phrasified_text, language, gender)\n        phonified_text = self.post_processor.textProcesor_list(phonified_text)\n        return phonified_text, phrasified_text\n\n\nclass CharTextPreprocessor(SharedInit):\n    # def __init__(self,\n    #             text_cleaner = TextCleaner(),\n    #             text_normalizer=TextNormalizer()):\n    #     self.text_cleaner = text_cleaner\n    #     self.text_normalizer = text_normalizer\n\n    def preprocess(self, text, language, gender=None, phone_dictionary=None):\n        text = text.strip()\n        text = self.text_normalizer.numberToTextConverter(text, language)\n        text = self.text_cleaner.clean(text)\n        # text = self.text_normalizer.insert_space(text)\n        #text = self.text_normalizer.num2text(text, language)\n        text = self.text_normalizer.normalize(text, language)\n        phrasified_text = TextPhrasifier.phrasify(text)\n        phonified_text = phrasified_text # No phonification for character TTS models\n        return phonified_text, phrasified_text\n\nclass CharTextPreprocessor_VTT(SharedInit):\n    # def __init__(self,\n    #             text_cleaner = TextCleaner(),\n    #             text_normalizer=TextNormalizer()\n    #             ):\n    #     self.text_cleaner = text_cleaner\n    #     self.text_normalizer = text_normalizer\n\n    def preprocess(self, text, language, gender=None):\n        # text = text.strip()\n        text = self.text_cleaner.clean_list(text)\n        # text = self.text_normalizer.insert_space_list(text)\n        text = self.text_normalizer.num2text_list(text, language)\n        text = self.text_normalizer.normalize_list(text, language)\n        phrasified_text = TextPhrasifier_List.phrasify(text)\n        phonified_text = phrasified_text # No phonification for character TTS models\n        return phonified_text, phrasified_text\n\n\nclass TTSPreprocessor(SharedInit):\n    # def __init__(self,\n    #             text_cleaner = TextCleaner(),\n    #             text_normalizer=TextNormalizer(),\n    #             phonifier = Phonifier(),\n    #             text_phrasefier = TextPhrasifier(),\n    #             post_processor = DurAlignTextProcessor()):\n    #     self.text_cleaner = text_cleaner\n    #     self.text_normalizer = text_normalizer\n    #     self.phonifier = phonifier\n    #     self.text_phrasefier = text_phrasefier\n    #     self.post_processor = post_processor\n        \n    def preprocess(self, text, language, gender, phone_dictionary):\n        text = text.strip()\n        text = self.text_normalizer.numberToTextConverter(text, language)\n        text = self.text_cleaner.clean(text)\n        # text = self.text_normalizer.insert_space(text)\n        #text = self.text_normalizer.num2text(text, language)\n        text = self.text_normalizer.normalize(text, language)\n        phrasified_text = TextPhrasifier.phrasify(text)\n        if language not in list(phone_dictionary.keys()):\n            phone_dictionary = self.phonifier.load_lang_dict(language, phone_dictionary)\n        phonified_text = self.phonifier.phonify(phrasified_text, language, gender, phone_dictionary)\n        #print(phonified_text)\n        phonified_text = self.post_processor.textProcesorForEnglish(phonified_text)\n        #print(phonified_text)\n        return phonified_text, phrasified_text\n\nclass TTSPreprocessor_VTT(SharedInit):\n    # def __init__(self,\n    #             text_cleaner = TextCleaner(),\n    #             text_normalizer=TextNormalizer(),\n    #             phonifier = Phonifier(),\n    #             text_phrasefier = TextPhrasifier_List()):\n    #     self.text_cleaner = text_cleaner\n    #     self.text_normalizer = text_normalizer\n    #     self.phonifier = phonifier\n    #     self.text_phrasefier = text_phrasefier\n\n    def preprocess(self, text, language, gender):\n        # print(f\"Original text: {text}\")\n        text = self.text_cleaner.clean_list(text)\n        # print(f\"After text cleaner: {text}\")\n        # text = self.text_normalizer.insert_space_list(text)\n        # print(f\"After insert space: {text}\")\n        text = self.text_normalizer.num2text_list(text, language)\n        # print(f\"After num2text: {text}\")\n        text = self.text_normalizer.normalize_list(text, language)\n        # print(f\"After text normalizer: {text}\")\n        phrasified_text = TextPhrasifier_List.phrasify(text)\n        # print(f\"phrasified_text: {phrasified_text}\")\n        phonified_text = self.phonifier.phonify_list(phrasified_text, language, gender)\n        # print(f\"phonified_text: {phonified_text}\")\n        return phonified_text, phrasified_text\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:33:10.837621Z","iopub.execute_input":"2024-08-24T01:33:10.837987Z","iopub.status.idle":"2024-08-24T01:33:15.969949Z","shell.execute_reply.started":"2024-08-24T01:33:10.837957Z","shell.execute_reply":"2024-08-24T01:33:15.968568Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Overwriting text_preprocess_for_inference.py\n","output_type":"stream"}]},{"cell_type":"code","source":"from text_preprocess_for_inference import TTSDurAlignPreprocessor,TTSPreprocessor\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:33:15.971919Z","iopub.execute_input":"2024-08-24T01:33:15.972383Z","iopub.status.idle":"2024-08-24T01:33:17.921168Z","shell.execute_reply.started":"2024-08-24T01:33:15.972340Z","shell.execute_reply":"2024-08-24T01:33:17.920009Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Loading G2P model... Done!\n","output_type":"stream"}]},{"cell_type":"code","source":"text='''जलवायु परिवर्तन से निपटने के लिए वैश्विक प्रयास तेजी से बढ़ रहे हैं, क्योंकि दुनिया भर के देश कार्बन उत्सर्जन को कम करने का संकल्प ले रहे हैं। संयुक्त राष्ट्र ने औद्योगिक क्रांति से पहले के स्तरों की तुलना में वैश्विक तापमान वृद्धि को 1.5 डिग्री सेल्सियस तक सीमित करने के लिए त्वरित कार्रवाई की अपील की है, और सरकारें इस दिशा में महत्वाकांक्षी योजनाएं बना रही हैं।\n\nतकनीकी क्षेत्र में, कृत्रिम बुद्धिमत्ता (ऐआई) के क्षेत्र में महत्वपूर्ण प्रगति हो रही है, जो विभिन्न उद्योगों के भविष्य को आकार दे रही है। स्वास्थ्य सेवाओं से लेकर वित्त तक, ऐआई व्यवसायों के संचालन के तरीकों को बदल रहा है, जिससे नए अवसर और नवाचार की संभावनाएं सामने आ रही हैं।\n\nखेेल जगत में, आगामी ओलंपिक खेलों का आयोजन दर्शकों के लिए बेहद रोमांचक होने वाला है। खिलाड़ी वैश्विक मंच पर प्रतिस्पर्धा के लिए तैयार हैं, अपनी भावनाओं और दृढ़ संकल्प को प्रदर्शित करते हुए। यह आयोजन एक रोमांचक नजारा होगा, जिसमें नए रिकॉर्ड्स बनने की उम्मीद है।\n\nअंत में, मनोरंजन की दुनिया में, एक बहुप्रतीक्षित फिल्म ने बॉक्स ऑफिस पर रिकॉर्ड्स तोड़ दिए हैं। इस फिल्म में अत्यधिक विशेष प्रभाव और एक सितारे से सजी कास्ट है, जिसमें समीक्षकों और दर्शकों दोनों ने प्रशंसा प्राप्त की है। '''","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:33:17.922735Z","iopub.execute_input":"2024-08-24T01:33:17.923307Z","iopub.status.idle":"2024-08-24T01:33:17.930491Z","shell.execute_reply.started":"2024-08-24T01:33:17.923277Z","shell.execute_reply":"2024-08-24T01:33:17.929362Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.argv=[\"tts.py\",\"--language\",\"hindi\",\"--gender\",\"female\",\"--sample_text\",text,\"--output_file\",\"output.wav\",\"--alpha\",\"1.2\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:37:18.479519Z","iopub.execute_input":"2024-08-24T01:37:18.479946Z","iopub.status.idle":"2024-08-24T01:37:18.485603Z","shell.execute_reply.started":"2024-08-24T01:37:18.479913Z","shell.execute_reply":"2024-08-24T01:37:18.484399Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"parser=argparse.ArgumentParser(description=\"Text-to-Speech inference\")\nparser.add_argument(\"--language\",type=str,required=True,help=\"Language(E.g Hindi)\")\nparser.add_argument(\"--gender\",type=str,required=True,help=\"Gender(E.g Female)\")\nparser.add_argument(\"--sample_text\",type=str,required=True,help=\"Text to be synthesized\")\nparser.add_argument(\"--output_file\",type=str,default='',help=\"Output Wav file path\")\nparser.add_argument(\"--alpha\",type=float,default=1,help=\"alpha parameter\")\nargs=parser.parse_args()\n\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nvocoder = load_hifigan_vocoder(args.language, args.gender, device)\n\npreprocessor = TTSDurAlignPreprocessor()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:37:20.110825Z","iopub.execute_input":"2024-08-24T01:37:20.111726Z","iopub.status.idle":"2024-08-24T01:37:20.528929Z","shell.execute_reply.started":"2024-08-24T01:37:20.111692Z","shell.execute_reply":"2024-08-24T01:37:20.527851Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n","output_type":"stream"},{"name":"stdout","text":"Removing weight norm...\n","output_type":"stream"}]},{"cell_type":"code","source":"import concurrent.futures\nimport numpy as np\nimport time\nimport base64\nimport yaml\nfrom scipy.io.wavfile import write\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:33:21.655825Z","iopub.execute_input":"2024-08-24T01:33:21.656144Z","iopub.status.idle":"2024-08-24T01:33:21.702464Z","shell.execute_reply.started":"2024-08-24T01:33:21.656117Z","shell.execute_reply":"2024-08-24T01:33:21.701096Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"os.remove('/kaggle/working/Fastspeech2_HS/output.wav')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:37:22.902329Z","iopub.execute_input":"2024-08-24T01:37:22.902694Z","iopub.status.idle":"2024-08-24T01:37:22.907726Z","shell.execute_reply.started":"2024-08-24T01:37:22.902666Z","shell.execute_reply":"2024-08-24T01:37:22.906719Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"!pip install -U librosa","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:33:43.258977Z","iopub.execute_input":"2024-08-24T01:33:43.259378Z","iopub.status.idle":"2024-08-24T01:33:57.606414Z","shell.execute_reply.started":"2024-08-24T01:33:43.259346Z","shell.execute_reply":"2024-08-24T01:33:57.605211Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.10.2.post1)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\nRequirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.23.5)\nRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.11.4)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.58.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3.7)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.9.0)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.7)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.41.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (21.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pooch>=1.1->librosa) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"start_time=time.time()\naudio_arr=[]\nresult=split_into_chunks(args.sample_text)\nphone_dictionary = {}\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    for sample_text in result:\n        preprocessed_text,phrases=preprocessor.preprocess(sample_text,args.language,args.gender,phone_dictionary)\n        \n        preprocessed_text=\" \".join(preprocessed_text)\n        \n        audio=text_synthesis(args.language,args.gender,preprocessed_text,vocoder,MAX_WAV_VALUE,device,args.alpha)\n        \n        if args.output_file:\n            output_file=f\"{args.output_file}\"\n        else:\n            output_file=f\"{args.language}_{args.gender}_output.wav\"\n        \n        audio_arr.append(audio)\nresult_array=np.concatenate(audio_arr,axis=0)\nwrite(output_file,SAMPLING_RATE,result_array)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T01:37:25.534300Z","iopub.execute_input":"2024-08-24T01:37:25.535243Z","iopub.status.idle":"2024-08-24T01:37:33.823130Z","shell.execute_reply.started":"2024-08-24T01:37:25.535178Z","shell.execute_reply":"2024-08-24T01:37:33.821906Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"language hindi\ndict_file_path phone_dict/hindi\nword not in dict: []\nTTS done\nword not in dict: []\nTTS done\nword not in dict: []\nTTS done\n","output_type":"stream"}]}]}